{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experimentation_notebook.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMz9ItUtWDm36pBk8awBXX/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_f4kaFmr9EZ","executionInfo":{"status":"ok","timestamp":1638263437240,"user_tz":300,"elapsed":37028,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}},"outputId":"748053f4-0c28-4fad-ac8f-14d66e91f8f2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlsuvikssCn0","executionInfo":{"status":"ok","timestamp":1638263478926,"user_tz":300,"elapsed":369,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}},"outputId":"be45020e-3059-479e-9de3-d21e414f0d61"},"source":["%cd drive/MyDrive/APCOMP215/"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/APCOMP215\n"]}]},{"cell_type":"code","metadata":{"id":"yolhMhUxoQ5r","executionInfo":{"status":"ok","timestamp":1638263534209,"user_tz":300,"elapsed":510,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["#!pip install luigi\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"97L3Yplvsy7g","executionInfo":{"status":"ok","timestamp":1638263538613,"user_tz":300,"elapsed":340,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["#!pip install mlflow"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6okIva-Sj9c","executionInfo":{"status":"ok","timestamp":1638263644208,"user_tz":300,"elapsed":351,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["import json\n","import logging\n","import os\n","import pathlib\n","import shutil\n","\n","##import ciri_pipeline.settings as settings\n","import luigi\n","import mlflow\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","##from ciri_pipeline.io.tasks import DownloadTrainingFilesTask\n","#from ciri_pipeline.settings import PIPELINE_DATA_DIR, VALIDATION_PCNT\n","from keras.callbacks import EarlyStopping\n","from numpy.random import choice\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","\n","\n","#from dotenv import load_dotenv\n","\n","#load_dotenv()\n","\n","# Define IO configurations:\n","PIPELINE_DATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\n","GCS_PROJECT_NAME = os.environ.get(\"GCS_PROJECT_NAME\", \"CIRI\")\n","GCS_DATA_BUCKET = os.environ.get(\"GCS_DATA_BUCKET\", \"canirecycleit-data\")\n","NUM_SHARDS = os.environ.get(\"NUM_SHARDS\", 10)\n","\n","# Define ML configurations:\n","VALIDATION_PCNT = os.environ.get(\"VALIDATION_PCNT\", 0.2)\n","BATCH_SIZE = os.environ.get(\"BATCH_SIZE\", 128)\n","IMAGE_WIDTH = os.environ.get(\"IMAGE_WIDTH\", 256)\n","IMAGE_HEIGHT = os.environ.get(\"IMAGE_HEIGHT\", 192)\n","\n","import logging\n","import os\n","\n","#import luigi\n","#from ciri_pipeline.settings import GCS_DATA_BUCKET, GCS_PROJECT_NAME, PIPELINE_DATA_DIR\n","from google.cloud import storage\n","\n","\n","class DownloadTrainingFilesTask(luigi.Task):\n","    \"\"\"Downloads files from CIRC GCS Bucket.\"\"\"\n","\n","    _data_dir = PIPELINE_DATA_DIR + \"/raw\"\n","    _gcs_project_name = GCS_PROJECT_NAME\n","    _gcs_bucket_name = GCS_DATA_BUCKET\n","\n","    def output(self):\n","        return luigi.LocalTarget(self._data_dir)\n","\n","    def run(self):\n","        # Anonymous client is used as bucket is public-read:\n","        storage_client = storage.Client.create_anonymous_client()\n","        bucket = storage_client.bucket(self._gcs_bucket_name)\n","\n","        # Find all content in bucket:\n","        blobs = bucket.list_blobs()\n","\n","        \n","\n","        # Create data-folder:\n","        os.makedirs(self._data_dir, exist_ok=True)\n","\n","        # Download blob files:\n","        for blob in blobs:\n","            #print(blob)\n","            destination = os.path.join(self._data_dir, blob.name)\n","            #print(destination)\n","            directory = os.path.dirname(os.path.abspath(destination))\n","\n","            # Create sub-directories:\n","            if not os.path.exists(directory):\n","                os.makedirs(directory, exist_ok=True)\n","\n","            if not blob.name.endswith(\"/\"):\n","                blob.download_to_filename(destination)\n","                logging.info(f\"Downloaded: {blob.name}\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1LruTypHmCL","executionInfo":{"status":"ok","timestamp":1638264836297,"user_tz":300,"elapsed":1189314,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["ciri_download = DownloadTrainingFilesTask()\n","ciri_download.run()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"0m7B1f1dfj8R","executionInfo":{"status":"ok","timestamp":1638264890087,"user_tz":300,"elapsed":420,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["class SplitTrainingValidationTask(luigi.Task):\n","    \"\"\"Splits files into training and validation folders.\"\"\"\n","\n","    _data_dir = PIPELINE_DATA_DIR\n","    _validation_percent = VALIDATION_PCNT\n","\n","    _validation_output = \"raw_validation\"\n","    _training_output = \"raw_training\"\n","    _label_map_output_folder = \"label_map\"\n","    _label_map_output_file = \"mapping.json\"\n","\n","    def requires(self):\n","        return DownloadTrainingFilesTask()\n","\n","    def output(self):\n","        train_output = os.path.join(self._data_dir, self._training_output)\n","        validation_output = os.path.join(self._data_dir, self._validation_output)\n","        label_map_output = os.path.join(\n","            self._data_dir, self._label_map_output_folder, self._label_map_output_file\n","        )\n","\n","        # 3 Outputs including 1) training data, 2) validation data\n","        # & a 3) map of label values:\n","        return [\n","            luigi.LocalTarget(train_output),\n","            luigi.LocalTarget(validation_output),\n","            luigi.LocalTarget(label_map_output),\n","        ]\n","\n","    def serialize_label_mapping(self, label_dict):\n","        \"\"\"Serializes the label mapping for later Task usage.\"\"\"\n","        # Create label-mapping folder:\n","        label_mapping_output_folder = os.path.join(\n","            self._data_dir, self._label_map_output_folder\n","        )\n","        os.makedirs(label_mapping_output_folder, exist_ok=True)\n","\n","        # Serialize mapping:\n","        label_mapping_file = os.path.join(\n","            label_mapping_output_folder, self._label_map_output_file\n","        )\n","        with open(label_mapping_file, \"w\") as f:\n","            f.write(json.dumps(label_dict))\n","            logging.info(\"Serialized mapping.json file.\")\n","\n","    def run(self):\n","\n","        # Get downloaded raw data:\n","        input_folder = self.input().path\n","\n","        # Get existing labels (via sub-folders):\n","        label_names = os.listdir(input_folder)\n","        label2index = dict((name, index) for index, name in enumerate(label_names))\n","\n","        # Create a label look-up for later Tasks:\n","        index2label = dict((index, name) for index, name in enumerate(label_names))\n","        self.serialize_label_mapping(index2label)\n","\n","        # Create training/validation output folders:\n","        os.makedirs(os.path.join(self._data_dir, self._training_output))\n","        os.makedirs(os.path.join(self._data_dir, self._validation_output))\n","\n","        # Iterate through all files and seperate:\n","        for root, dirs, files in os.walk(input_folder):\n","            for filename in files:\n","\n","                # Convert to index:\n","                sub_dir = pathlib.PurePath(root).name\n","                label_dir = str(label2index[sub_dir])\n","\n","                draw = choice(\n","                    [self._training_output, self._validation_output],\n","                    1,\n","                    p=[1 - self._validation_percent, self._validation_percent],\n","                )[0]\n","\n","                destination_dir = os.path.join(self._data_dir, draw, label_dir)\n","                if not os.path.exists(destination_dir):\n","                    os.makedirs(destination_dir, exist_ok=True)\n","\n","                # Copy file to assigned folder:\n","                shutil.copy(\n","                    os.path.join(root, filename),\n","                    os.path.join(destination_dir, filename),\n","                )\n","\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjVL07CHI-tc","executionInfo":{"status":"ok","timestamp":1638264922130,"user_tz":300,"elapsed":28308,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["ciri_split = SplitTrainingValidationTask()\n","ciri_split.run()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"-jjQHDWuoEtY","executionInfo":{"status":"ok","timestamp":1638265771615,"user_tz":300,"elapsed":522,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["class BuildTFRecordTask(luigi.Task):\n","    \"\"\"Serialize Image Files as TFRecords.\"\"\"\n","\n","    _batch_size = BATCH_SIZE\n","    _image_width = IMAGE_WIDTH\n","    _image_height = IMAGE_HEIGHT\n","    _num_channels = 3\n","\n","    _num_shards = NUM_SHARDS\n","\n","    _data_dir = PIPELINE_DATA_DIR\n","    _folder_output = \"tfrecord_output\"\n","\n","    # Selects which index from SplitTrainingValidationTask\n","    # to use (0=Training, 1=Validation)\n","    _input_index = 0\n","\n","    def requires(self):\n","        return SplitTrainingValidationTask()\n","\n","    def output(self):\n","        tfrecord_output_folder = os.path.join(self._data_dir, self._folder_output)\n","        return luigi.LocalTarget(tfrecord_output_folder)\n","\n","    def create_tf_example(self, item):\n","        \"\"\"Create a TFRecord Example for serialization.\n","        param: item - tuple (label, file_path)\n","        \"\"\"\n","\n","        file_path = item[1]\n","        label = int(item[0])\n","\n","        # Read image\n","        image = tf.io.read_file(file_path)\n","        image = tf.image.decode_jpeg(image, channels=self._num_channels)\n","        image = tf.image.resize(image, [self._image_height, self._image_width])\n","        image = tf.cast(image, tf.uint8)\n","\n","        # Build feature dict\n","        feature_dict = {\n","            \"image\": tf.train.Feature(\n","                bytes_list=tf.train.BytesList(value=[image.numpy().tobytes()])\n","            ),\n","            \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n","            \"width\": tf.train.Feature(\n","                int64_list=tf.train.Int64List(value=[self._image_width])\n","            ),\n","            \"height\": tf.train.Feature(\n","                int64_list=tf.train.Int64List(value=[self._image_height])\n","            ),\n","        }\n","\n","        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n","        return example\n","\n","    def create_tf_records(self, data, num_shards=10, prefix=\"\", folder=\"data\"):\n","        \"\"\"Create TFRecord shards.\"\"\"\n","\n","        num_records = len(data)\n","        step_size = num_records // num_shards + 1\n","\n","        for i in range(0, num_records, step_size):\n","            logging.info(\n","                \"Creating shard:\" +\n","                str(i // step_size) +\n","                \" from records:\" +\n","                str(i) +\n","                \"to\" +\n","                str(i + step_size)\n","            )\n","            path = \"{}/{}_000{}.tfrecords\".format(folder, prefix, i // step_size)\n","            logging.info(path)\n","\n","            # Write the file\n","            with tf.io.TFRecordWriter(path) as writer:\n","                # Filter the subset of data to write to tfrecord file\n","                for item in data[i : i + step_size]:\n","                    tf_example = self.create_tf_example(item)\n","                    writer.write(tf_example.SerializeToString())\n","\n","    def run(self):\n","        logging.info(f\"Running: {self.__class__.__name__}\")\n","\n","        # Guard-statement:\n","        if self._input_index > 1:\n","            raise ValueError(\"_input_index must be less than 2\")\n","\n","        input_folder = self.input()[self._input_index].path\n","\n","        labels = os.listdir(input_folder)\n","        data_list = []\n","        for label in labels:\n","            image_files = os.listdir(os.path.join(input_folder, label))\n","            data_list.extend(\n","                [(label, os.path.join(input_folder, label, f)) for f in image_files]\n","            )\n","\n","        # Create output folder:\n","        os.makedirs(os.path.join(self._data_dir, self._folder_output))\n","\n","        # Create TF Records:\n","        self.create_tf_records(\n","            data_list,\n","            num_shards=self._num_shards,\n","            folder=os.path.join(self._data_dir, self._folder_output),\n","        )\n","\n","\n","class TrainTFRecordTask(BuildTFRecordTask):\n","\n","    _input_index = 0\n","    _folder_output = \"tfrecord_training\"\n","\n","\n","class ValidationTFRecordTask(BuildTFRecordTask):\n","\n","    _input_index = 1\n","    _folder_output = \"tfrecord_validation\"\n","\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"TE_vDJTkNbme","executionInfo":{"status":"ok","timestamp":1638265793906,"user_tz":300,"elapsed":16966,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["ciri_BuildTFRecord = BuildTFRecordTask()\n","ciri_BuildTFRecord.run()"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QI4O3CwQ3SK","executionInfo":{"status":"ok","timestamp":1638266063714,"user_tz":300,"elapsed":16481,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["ciri_TrainTFRecordTask = TrainTFRecordTask()\n","ciri_TrainTFRecordTask.run()"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhgBFpBPQ3jz","executionInfo":{"status":"ok","timestamp":1638266079762,"user_tz":300,"elapsed":4198,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["ciri_ValidationTFRecordTask = ValidationTFRecordTask()\n","ciri_ValidationTFRecordTask.run()"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"vot_CzkNp8q_","executionInfo":{"status":"ok","timestamp":1638271397457,"user_tz":300,"elapsed":358,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["class TrainModel(luigi.Task):\n","\n","    _batch_size = BATCH_SIZE\n","\n","    _image_width = IMAGE_WIDTH\n","    _image_height = IMAGE_HEIGHT\n","    _num_channels = 3\n","\n","    _data_dir = PIPELINE_DATA_DIR\n","    _output_folder = \"trained_model\"\n","\n","    _feature_description = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string),\n","        \"label\": tf.io.FixedLenFeature([], tf.int64),\n","        \"width\": tf.io.FixedLenFeature([], tf.int64),\n","        \"height\": tf.io.FixedLenFeature([], tf.int64),\n","    }\n","\n","    def requires(self):\n","        return [\n","            TrainTFRecordTask(),\n","            ValidationTFRecordTask(),\n","            SplitTrainingValidationTask(),\n","        ]\n","\n","    def output(self):\n","        output_folder = os.path.join(self._data_dir, self._output_folder)\n","        return [luigi.LocalTarget(output_folder)]\n","\n","    def parse_tfrecord(self, proto):\n","        \"\"\"Parses a serialized TF Record.\"\"\"\n","        parsed_record = tf.io.parse_single_example(proto, self._feature_description)\n","\n","        image = tf.io.decode_raw(parsed_record[\"image\"], tf.uint8)\n","        image.set_shape([self._num_channels * self._image_height * self._image_width])\n","        image = tf.reshape(\n","            image, [self._image_height, self._image_width, self._num_channels]\n","        )\n","\n","        label = tf.cast(parsed_record[\"label\"], tf.int32)\n","\n","        return image, label\n","\n","    def normalize(self, image, label):\n","        \"\"\"Normalize pixels between 0 and 1.\"\"\"\n","        image = image / 255\n","        return image, label\n","\n","    def build_transfer_model(self, num_classes):\n","        \"\"\"Build a transfer model based on pre-trained architecture.\"\"\"\n","        input_shape = [\n","            self._image_height,\n","            self._image_width,\n","            self._num_channels,\n","        ]\n","\n","        #handle = (\n","        #    \"https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/5\")\n","\n","\n","        #Models trained on ImageNet-21K and finetuned on ImageNet-1K\n","        #handle = (\n","        #    \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2\"\n","        #)\n","\n","        #Models trained on ImageNet-21K and finetuned on ImageNet-1K\n","        handle = (\n","            \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/classification/2\"\n","        )\n","        # Regularize using L1\n","        kernel_weight = 0.0001 #0.02 changend by Dan\n","        bias_weight = 0.02\n","\n","        model = Sequential(\n","            [\n","                keras.layers.InputLayer(input_shape=input_shape),\n","                hub.KerasLayer(handle, trainable=False),\n","                keras.layers.Dense(\n","                    units=124,\n","                    activation=\"relu\",\n","                    kernel_regularizer=keras.regularizers.l1(kernel_weight),\n","                    #bias_regularizer=keras.regularizers.l1(bias_weight), #commented out by dan\n","                ),\n","                keras.layers.Dropout(rate=0.2), # added by dan\n","                keras.layers.Dense(\n","                    units=64,\n","                    activation=\"relu\",\n","                    kernel_regularizer=keras.regularizers.l1(kernel_weight),\n","                    #bias_regularizer=keras.regularizers.l1(bias_weight), #commented out by dan\n","                ),\n","                keras.layers.Dropout(rate=0.2), #added by Dan\n","                keras.layers.Dense(\n","                    units=num_classes,\n","                    activation=None,\n","                    kernel_regularizer=keras.regularizers.l1(kernel_weight),\n","                    #bias_regularizer=keras.regularizers.l1(bias_weight), #commented out by dan\n","                ),\n","            ],\n","            name=\"transfer_model\",\n","        )\n","\n","        return model\n","\n","    def build_pipeline(self, tfrecordfiles, augment=False):\n","        \"\"\"Builds pipeline from TFRecord\"\"\"\n","\n","        AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","        # Optional image augmentation layer:\n","        data_augmentation = tf.keras.Sequential(\n","            [\n","                layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n","                layers.experimental.preprocessing.RandomRotation(0.2),\n","                layers.experimental.preprocessing.RandomZoom(0.2, 0.2),\n","            ]\n","        )\n","\n","        data = tfrecordfiles.flat_map(tf.data.TFRecordDataset)\n","        data = data.map(self.parse_tfrecord, num_parallel_calls=AUTOTUNE)\n","        data = data.map(self.normalize, num_parallel_calls=AUTOTUNE)\n","\n","        if augment:\n","            data = data.map(\n","                lambda x, y: (data_augmentation(x, training=True), y),\n","                num_parallel_calls=AUTOTUNE,\n","            )\n","\n","        data = data.batch(self._batch_size)\n","        data = data.prefetch(buffer_size=AUTOTUNE)\n","        return data\n","\n","    def run(self):\n","        \"\"\"Build data pipeline and train resulting model.\"\"\"\n","\n","        training_input_folder = self.input()[0].path\n","        validation_input_folder = self.input()[1].path\n","\n","        index2label = {}\n","        label_mapping = self.input()[-1][2].path\n","        with open(label_mapping, \"r\") as f:\n","            index2label = json.loads(f.read())\n","\n","        num_classes = len(index2label)\n","        \n","        print(\"number of classes:\",num_classes)\n","\n","        train_tfrecord_files = tf.data.Dataset.list_files(training_input_folder + \"/*\")\n","        validation_tfrecord_files = tf.data.Dataset.list_files(\n","            validation_input_folder + \"/*\"\n","        )\n","\n","        train_data = self.build_pipeline(train_tfrecord_files)\n","        validation_data = self.build_pipeline(validation_tfrecord_files)\n","\n","        # Model Training Parameters\n","        learning_rate = 0.01\n","        decay_rate = 0.5\n","        epochs = 15\n","\n","        # Model parameters\n","        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9) #dan added momentum\n","        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) # dan added label smoothing\n","        es = EarlyStopping(monitor=\"val_accuracy\", verbose=1, patience=3)\n","        lr = keras.callbacks.LearningRateScheduler(\n","            lambda epoch: learning_rate / (1 + decay_rate * epoch)\n","        )\n","\n","        #with mlflow.start_run():\n","\n","        # Execute different model approaches and save experiment results:\n","        model = self.build_transfer_model(num_classes=num_classes)\n","\n","        print(model.summary())\n","        model.compile(\n","            loss=loss,\n","            optimizer=optimizer,\n","            metrics=[\"accuracy\", \"sparse_categorical_accuracy\"],\n","        )\n","\n","        # Train model\n","        training_results = model.fit(\n","            train_data,\n","            validation_data=validation_data,\n","            epochs=epochs,\n","            callbacks=[es, lr],\n","            verbose=1,\n","            )\n","        \n","        model.save('efficientnet_ciri_b1.h5')\n","\n","         "],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAL__gV_qZWF","executionInfo":{"status":"ok","timestamp":1638271401576,"user_tz":300,"elapsed":356,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}}},"source":["ciri = TrainModel()"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o311MsgVqcgH","executionInfo":{"status":"ok","timestamp":1638271683774,"user_tz":300,"elapsed":280064,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}},"outputId":"4ed8b696-8d2f-4751-c121-98424311cea4"},"source":["ciri.run()"],"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["number of classes: 7\n","Model: \"transfer_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," keras_layer_4 (KerasLayer)  (None, 1000)              8212124   \n","                                                                 \n"," dense_12 (Dense)            (None, 124)               124124    \n","                                                                 \n"," dropout_6 (Dropout)         (None, 124)               0         \n","                                                                 \n"," dense_13 (Dense)            (None, 64)                8000      \n","                                                                 \n"," dropout_7 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_14 (Dense)            (None, 7)                 455       \n","                                                                 \n","=================================================================\n","Total params: 8,344,703\n","Trainable params: 132,579\n","Non-trainable params: 8,212,124\n","_________________________________________________________________\n","None\n","Epoch 1/15\n","21/21 [==============================] - 22s 614ms/step - loss: 2.9642 - accuracy: 0.3418 - sparse_categorical_accuracy: 0.3418 - val_loss: 2.5067 - val_accuracy: 0.4441 - val_sparse_categorical_accuracy: 0.4441 - lr: 0.0100\n","Epoch 2/15\n","21/21 [==============================] - 11s 538ms/step - loss: 2.2957 - accuracy: 0.3865 - sparse_categorical_accuracy: 0.3865 - val_loss: 1.6662 - val_accuracy: 0.6157 - val_sparse_categorical_accuracy: 0.6157 - lr: 0.0067\n","Epoch 3/15\n","21/21 [==============================] - 11s 535ms/step - loss: 1.7134 - accuracy: 0.5703 - sparse_categorical_accuracy: 0.5703 - val_loss: 1.3559 - val_accuracy: 0.7181 - val_sparse_categorical_accuracy: 0.7181 - lr: 0.0050\n","Epoch 4/15\n","21/21 [==============================] - 12s 553ms/step - loss: 1.5532 - accuracy: 0.6586 - sparse_categorical_accuracy: 0.6586 - val_loss: 1.3869 - val_accuracy: 0.6677 - val_sparse_categorical_accuracy: 0.6677 - lr: 0.0040\n","Epoch 5/15\n","21/21 [==============================] - 11s 535ms/step - loss: 1.6056 - accuracy: 0.6211 - sparse_categorical_accuracy: 0.6211 - val_loss: 1.4723 - val_accuracy: 0.7071 - val_sparse_categorical_accuracy: 0.7071 - lr: 0.0033\n","Epoch 6/15\n","21/21 [==============================] - 11s 534ms/step - loss: 1.5300 - accuracy: 0.6741 - sparse_categorical_accuracy: 0.6741 - val_loss: 1.3361 - val_accuracy: 0.7528 - val_sparse_categorical_accuracy: 0.7528 - lr: 0.0029\n","Epoch 7/15\n","21/21 [==============================] - 11s 535ms/step - loss: 1.3125 - accuracy: 0.7397 - sparse_categorical_accuracy: 0.7397 - val_loss: 1.2113 - val_accuracy: 0.7732 - val_sparse_categorical_accuracy: 0.7732 - lr: 0.0025\n","Epoch 8/15\n","21/21 [==============================] - 11s 534ms/step - loss: 1.2806 - accuracy: 0.7207 - sparse_categorical_accuracy: 0.7207 - val_loss: 1.1568 - val_accuracy: 0.8315 - val_sparse_categorical_accuracy: 0.8315 - lr: 0.0022\n","Epoch 9/15\n","21/21 [==============================] - 11s 535ms/step - loss: 1.2026 - accuracy: 0.7658 - sparse_categorical_accuracy: 0.7658 - val_loss: 1.0917 - val_accuracy: 0.8331 - val_sparse_categorical_accuracy: 0.8331 - lr: 0.0020\n","Epoch 10/15\n","21/21 [==============================] - 11s 536ms/step - loss: 1.2114 - accuracy: 0.7389 - sparse_categorical_accuracy: 0.7389 - val_loss: 1.0951 - val_accuracy: 0.8220 - val_sparse_categorical_accuracy: 0.8220 - lr: 0.0018\n","Epoch 11/15\n","21/21 [==============================] - 11s 535ms/step - loss: 1.1453 - accuracy: 0.7912 - sparse_categorical_accuracy: 0.7912 - val_loss: 1.0507 - val_accuracy: 0.8441 - val_sparse_categorical_accuracy: 0.8441 - lr: 0.0017\n","Epoch 12/15\n","21/21 [==============================] - 11s 534ms/step - loss: 1.1469 - accuracy: 0.7795 - sparse_categorical_accuracy: 0.7795 - val_loss: 1.0546 - val_accuracy: 0.8220 - val_sparse_categorical_accuracy: 0.8220 - lr: 0.0015\n","Epoch 13/15\n","21/21 [==============================] - 11s 536ms/step - loss: 1.1607 - accuracy: 0.7507 - sparse_categorical_accuracy: 0.7507 - val_loss: 1.0349 - val_accuracy: 0.8362 - val_sparse_categorical_accuracy: 0.8362 - lr: 0.0014\n","Epoch 14/15\n","21/21 [==============================] - 11s 534ms/step - loss: 1.0829 - accuracy: 0.8102 - sparse_categorical_accuracy: 0.8102 - val_loss: 1.0189 - val_accuracy: 0.8472 - val_sparse_categorical_accuracy: 0.8472 - lr: 0.0013\n","Epoch 15/15\n","21/21 [==============================] - 11s 532ms/step - loss: 1.0514 - accuracy: 0.8211 - sparse_categorical_accuracy: 0.8211 - val_loss: 1.0059 - val_accuracy: 0.8520 - val_sparse_categorical_accuracy: 0.8520 - lr: 0.0012\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYI3N4QSQNcy","executionInfo":{"status":"ok","timestamp":1638265843692,"user_tz":300,"elapsed":426,"user":{"displayName":"Daniel Olal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04917530161673316794"}},"outputId":"954603f9-2653-48fd-d54c-9c51e06d1b0d"},"source":["ls"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["02_tutorial_data_dask_cloud_store.ipynb       \u001b[0m\u001b[01;34mdatasets\u001b[0m/\n","03_tutorial_data_tfdata_tfrecords.ipynb       dolal_exercise_2.ipynb\n","04_demo_mushroom_classification_models.ipynb  dolal_exercise_3_final.ipynb\n","04_tutorial_model_compression.ipynb           dolal_exercise_4.ipynb\n","05_tutorial_text_classification.ipynb         eda.ipynb\n","05_tutorial_text_generation.ipynb             experimentation_notebook.ipynb\n","ciri.h5                                       \u001b[01;34mmlruns\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/                                         run_mlm.py\n","\u001b[01;34mdata_old\u001b[0m/                                     \u001b[01;34mwaste_classification_dataset\u001b[0m/\n","\u001b[01;34mdataset-resized\u001b[0m/                              waste_classification_models.ipynb\n"]}]}]}